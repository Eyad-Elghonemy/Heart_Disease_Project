=======================================
 Supervised Models (Baseline - Clean)
=======================================
Model              Accuracy   Precision   Recall   F1     ROC AUC
-----------------------------------------------------------------
Logistic Regression 0.8852    0.8854      0.8852  0.8851  0.9632
Decision Tree       0.6721    0.6746      0.6721  0.6727  0.6824
Random Forest       0.7869    0.7914      0.7869  0.7872  0.9004
SVM                 0.8525    0.8524      0.8525  0.8522  0.9448

=======================================
 Supervised Models (Baseline - PCA)
=======================================
Model              Accuracy   Precision   Recall   F1     ROC AUC
-----------------------------------------------------------------
Logistic Regression 0.8852    0.8860      0.8852  0.8854  0.9437
Decision Tree       0.7377    0.7401      0.7377  0.7381  0.7386
Random Forest       0.8033    0.8035      0.8033  0.8025  0.9091
SVM                 0.8525    0.8532      0.8525  0.8526  0.9502

=======================================
 Unsupervised Models (Clustering)
=======================================
KMeans (K=5):       ARI = 0.119,  NMI = 0.177
Hierarchical (K=2): ARI = 0.204,  NMI = 0.153
Hierarchical (K=5): ARI = 0.111,  NMI = 0.162

=> Clustering showed weak alignment with actual labels.

=======================================
 Hyperparameter Tuning Results
=======================================
Best Parameters (via GridSearchCV):
-----------------------------------
- Logistic Regression: C=1, penalty=l1, solver=liblinear (CV=0.818)
- Decision Tree: max_depth=10, min_samples_leaf=2, min_samples_split=5 (CV=0.793)
- Random Forest: n_estimators=100, max_depth=40, bootstrap=False, min_samples_split=10 (CV=0.810)
- SVM: kernel=linear, C=0.1, gamma=scale (CV=0.826)

Performance After Tuning:
-------------------------
Logistic Regression: Accuracy=0.9180, F1=0.9123
Decision Tree:       Accuracy=0.8197, F1=0.7755
Random Forest:       Accuracy=0.7705, F1=0.7500
SVM:                 Accuracy=0.8852, F1=0.8772

Baseline vs Tuned Comparison:
-----------------------------
| Model              | Accuracy Baseline | F1 Baseline | Accuracy Tuned | F1 Tuned |
|--------------------|-------------------|-------------|----------------|----------|
| Logistic Regression| 0.9180            | 0.9123      | 0.9180         | 0.9123   |
| Decision Tree      | 0.6885            | 0.6415      | 0.8197         | 0.7755   |
| Random Forest      | 0.7377            | 0.7143      | 0.7705         | 0.7500   |
| SVM                | 0.8689            | 0.8571      | 0.8852         | 0.8772   |

=======================================
 Final Conclusion
=======================================
- Logistic Regression consistently outperformed other models.
- PCA and Feature Selection provided insights but did not improve performance.
- Clustering methods performed poorly (weak ARI/NMI).
- Hyperparameter Tuning confirmed Logistic Regression as the best model
  (Accuracy = 91.8%, F1 = 91.2%).
- Final deployed model: **Logistic Regression**.
